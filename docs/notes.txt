Nestbox coordinate system alignment methods notes

From the perspective of one coordinate system, features that are known to be part of an object might move around until they best fit a local measurement. You could start with an initial guess, calculate the log likelihood based on the measurements, and then nudge the guess (gradient descent) until the likelihood is maximized. There should also be a log likelihood associated with the position of the coordinate system itself, but this would start with variances at infinity or max-float-value or such. The solution found could be quite far off though in the case of using a small part of an image as a measurement for example, or a symmetry that can't be resolved from one perspective. Then, another coordinate system can be introduced with its own set of measurements of the same features. There are a lot of degrees of freedom now--the new coordinate system can go anywhere. Still, you can start with a guess, and wherever it is, you can move the measurements into the local frame and calculate a log likelihood for that arrangement. Then you gradient-descend over both the positions of the features and the coordinate transform at the same time, until the new coordinate system's measurements line up on the features.

It should be possible to do this procedure equally from any coordinate system's perspective. One could pick one of the coordinate systems randomly, or even initialize a new random space for all to work in, but it might be good to prefer one that has geometry it expects to find at a particular location, e.g. a 3D model of "itself," a vision target, a particular trajectory something local is known to have followed that others may have observed, things like that.

One important thing to share might be coordinate systems' best estimates of other coordinate systems' transforms at a particular moment in time. This should be viewed as part of the convergence process, meaning that it will have extremely high uncertainties at first, and these should be honestly communicated and properly compounded with other uncertainties.

At any time, any coordinate system can communicate to any other, "I think you're *here* (full transform/parameter estimate) with *this* uncertainty: [..., ...]" I would think that this should be considered a pretty fully optimized current estimate, i.e. many rounds of gradient descent done, local minimum probably found, before sending, such that if this message were sent between two coordinate systems already estimated within one aligner, nothing would change. The receiver can then invert this and use it to refine its estimated state of the sending coordinate system; it's a different optimizer with a different communication history, so it's probably using different data from other instances, meaning that it is likely to be able to refine beyond what the sending coordinate system could do. It's tough to say how exactly this would be incorporated, but it might be possible to just assume that its variances are overestimates, calculate a log likelihood of the current transform/estimate and add that into the loss function.

It's possible there is a UKF component of the system associated with communication and updates between coordinate systems on other coordinate systems, and a maximum-log-likelihood component. The UKF could be important because many rounds of communication may take place, leading to an explosion of recursively defined gradients. The UKF would collapse this situation into one state estimate that is only updated one round at a time; its internally estimated distributions (variances) are then used to feed a round of the gradient descent component.

Received a pose estimate of self from other coordinate system? Invert, UKF update that coordinate system, and then set up gradient descent using the updated coordinate system's (ideally smaller) variances. start it using the UKF means as an initial guess for that coordinate system, and run to convergence. (having some other system intervene kind of invasively into the UKF might cause chaos but might be gentle enough to work.)
Received a measurement from other coordinate system? Just gradient descent to convergence with the new data.

It should be possible to incorporate measurements with very different numbers of dimensions within the framework. A camera focused at infinity essentially observes a line, not a point with an insanely elongated covariance. Position measurements of non-moving objects basically represent lines parallel to the time axis. A camera with no relative motion to its target actually observes a plane cutting through time and space simultaneously. Simple measurements with important symmetries can potentially cut down the space of solutions very fast.

In the initial prototype, I'm already treating time as non-existent, which is the same thing as projecting it out/marginalizing & normalizing before taking a log likelihood over the other (space) dimensions. I could iterate this and/or make a generic framework for projecting spacetime down to the right dimension before defining a gaussian over it. This could include a skew through space and time for a moving (but known constant-speed) target, covariance anisotropy in space dimensions, things like that. All of the factors could be included as linear terms up to the point where loss is calculated entirely as a (hyper-)spherically-symmetric gaussian, super computationally efficient. If I preemptively only allow transformations that preserve the speed of light (minkowski metric?) (which is well justified for things like cameras because light is what's measured, and it's assumed to be traveling in a straight line) before projection, I might get special relativity (time adjustment for relative motion) for free. A time synchronization event would just be another point measurement with all the space dimensions projected out. Clock skew is just time anisotropy.

For that matter, I'm also assuming non-rotation, implicitly projecting out any axes that might correspond. Angular velocity might also be subject to this type of formal projection--if SLERP is possible, with a well-defined quaternion analogy for "linear" "slope", can this dimension/axis be collapsed for nonzero angular velocities? Since I'm guessing it has some nonlinearity and periodicity for large rotations, should we assume a small rotation over an interval and apply a taylor expansion maybe? Is there a projective geometry for quaternions? Research topic. If this is possible, and done consistently, there can be an origin and "slope" through time estimate for rotations, calibrated and optimized just like clock skew. Maybe things like the rotation of the earth during the time it took light to travel from Mars can be included down at the model level.

Overall, if a measurement provides a covariance matrix for some of its dimensions, the implicit assumption is that all dimensions not included are marginalized. e.g. if time does not appear in the matrix, presumably it does not change, a la the current prototype. Could there be a projection matrix provided as context (that is often the identity or a diagonal matrix with only ones and zeros) so that a camera, for example, could supply its projection to the image plane and give its covariances in terms of the projected coordinates? This would neatly solve for things like the conical (not multiv-gaussian) uncertainty for images approx focused at infinity.

This is re-solidifying the importance of rigid sets of measurements. (a nest-themed name for this could be cute. A strand? Ply? Stick?) Previously I was looking at runs of points in time that are known to have correct relative/delta timing even if their global time alignment is not known, i.e. they could each start with time zero. This is good for fitting trajectories between coordinate systems. Now we can think more abstractly about "rigidity." a rigid set of measurements has known deltas between observations along all axes it measures. These can be the known shape of a target rigid object, i.e. a 3D model; the even (or at least accurate) spacing of time points; the projective coordinates within a single image (I think zero point down axis of camera/center of image can be assumed); or a combination, like all the times and points in a video that fireworks went off. All rigid sets of measurements are matrices, i.e. columns are individual events/points and rows are the relevant dimensions, and covariances over those dimensions only, with every entry filled in. They may be augmented with a projection matrix if the dimensions given are non-standard, e.g. camera projection from meters^3 to image space or streak camera (@_@) projection from meters and time to image space. Even doppler might have a projection going on (confounding between time and speed).

It should be possible to stitch rigid measurements together to identify certain points/events with others by time/space correlations, both within and between coordinate systems. This amounts to guessing a time, space, and rotation alignment--a microcosm of the whole optimization problem--but the difference is, one can throw out a whole rigid measurement if it isn't integrating right, while preserving the continuity of its originating coordinate system.

Some notes on cameras. Any time a camera minutely rotates, the projection trick probably breaks, so prob best to send points in shaky/mounted video as separated rigid measurements. Could you even provide it symbolically as one or more free variables if e.g. a camera's focal length is not known?? or provide that as a measurement with its own variance??? There are quite a few parameters that might be estimated in tandem: "optical center ð‘ð‘¥,ð‘ð‘¦ or ð‘ð‘¢,ð‘ð‘£; horizontal and vertical 'focal length':ð‘“ð‘¥,ð‘“ð‘¦ or ð‘“ð‘¢,ð‘“ð‘£; aspect ratio; field of view angle ð›¼; shear (you don't usually want that)." There's a choice in which to model. I guess measurements generically could come with free parameters.

Free parameters should be considered constants within one/each coordinate system, think physical properties like focal length, xyz offsets & rotations of rigidly mounted cameras' or sensor ensembles' perspectives. Otherwise this could get really crazy. Coordinate systems register them along with themselves, and can give measurements in terms of linear combinations of them.

Gradient descend-normalize alternation might be very inefficient. worth checking out. It might be possible to locally rewrite the parameters of the quaternion manifold as a tangent space by linearizing at the current point where moving into non-normalized quaternions is disallowed, and doing gradient descent over that instead. This 3D (not 4D) tangent space can (probably) be specified by saying the determinant of the jacobian of the linearizing coordinate change is zero, i.e. it's an envelope point. It's possible this will just spit out normal gradient descent again as a solution, but if it somehow generates a delta that always very nearly locates the new point in SO(3), it might speed up convergence a LOT.



Nestbox network/logistics/coordination brainstorming

Concrete example. All features static, not varying with time. No relative motion between frames. The objective is to move a plate on a liquid handling robot to a location on another robot, where the transform of the second robot is not known. the liquid handling robot has a realistic rubber left hand on it. It knows the position and orientation of the hand in its local coordinate system. The other robot has a rubber right hand and similarly knows its local pose. A VR tracking system can see both rubber hands but does not know where they are at all a priori. The VR system takes a few hundred measurements of the left hand pose ([position, orientation, variance in position, and covariance in quaternion] x ~1e2) and then takes a few hundred similar measurements of the right hand pose, reporting summaries of its measurements to a Nestbox host.

In more detail, the VR app starts up and loads a mapping from hand.chirality.left and hand.chirality.right to particular global Nestbox object names. It also loads up a known Nestbox host URL or local IP and its global Nestbox coordinate system name. It connects to wifi. A script is on both the left and right hands in Unity. It waits until tracking has been established for a certain time, and then calls Nestbox in C# to issue an HTTP request (PUT?) with the object identifier looked up with the chirality map and the measurement, along with an estimated covariance. The measurement is the position of this object in VR tracking space local coordinates, and an SO3 object, probably a quaternion, along with a covariance matrix that can be directly calculated from the quaternion component data using the covariance formula. After a delay, if it is still tracked, another similar HTTP request reporting new data is sent. Later, the same happens for the right hand, whenever it comes into view, possibly simultaneously.

PyLabRobot on robot 1 tries to set up the position of robot 2 on its lab resource layout. To get the XYZ position and orientation in space (probably an SO3 matrix in PLR?) it imports the nestbox module, pulling in some strings from a config file representing both its own coordinate system's global name in the nestbox system, the other robot's coordinate system name, and the global Nestbox name of the trackable hand object. It also gets a known Nestbox host URL or local IP. When the nestbox module initializes, it uses HTTP to tell the Nestbox host its coordinate system name, and registers its trackable hand object by name along with its "measurement" of its position relative to this coordinate system, which can safely have zero or epsilon variance. Nestbox uses the requests module to GET data about robot 2's coordinate system name, but there's nothing at the host about it yet, so it hangs.

Robot 2 may or may not have nestbox running. Preferably it does, and it's participating symmetrically. If so, when it starts up, it also registers its trackable hand object, and finally there is a chain of: coords robot 1 -> left hand -> VR tracking coords -> right hand -> coords robot 2. Robot 1 need not know the name of the right hand tracked object.
